{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from hkube_notebook import AlgorithmBuilder, PipelineBuilder, PipelineExecutor, TrackerType\n",
    "import hkube_notebook\n",
    "print(f'hkube_notebook {hkube_notebook.__version__}')\n",
    "\n",
    "api_server_local = 'http://localhost:3000/api/v1'\n",
    "api_server_test = 'https://10.32.10.11/hkube/api-server/api/v1'\n",
    "api_server_cd = 'https://40.69.222.75/hkube/api-server/api/v1'\n",
    "\n",
    "# select hkube cluster\n",
    "api_server = api_server_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create alg by function (all imports and internal functions must be nested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_bldr = AlgorithmBuilder(api_server_base_url=api_server)\n",
    "algs = alg_bldr.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_start(args):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    n = 1000\n",
    "    df = pd.DataFrame({'x': np.random.randint(0, 5, size=n), 'y': np.random.normal(size=n)})\n",
    "    print(df.columns)\n",
    "\n",
    "    input = args[\"input\"]\n",
    "    print(f'algorithm: start, input: {input}')\n",
    "    print('working...')\n",
    "    time.sleep(5)\n",
    "    array = input[0]\n",
    "    order = input[1]\n",
    "    if order == 'asc':\n",
    "        reverse = False\n",
    "    elif order == 'desc':\n",
    "        reverse = True\n",
    "    else:\n",
    "        raise Exception('order not supported')\n",
    "\n",
    "    list.sort(array, reverse=reverse)\n",
    "    return array\n",
    "\n",
    "worker_env = { \"WORKER_ALGORITHM_PROTOCOL\": \"ws\" }\n",
    "entry, tarfilename = alg_bldr.create_algfile_by_functions(my_start)\n",
    "config = alg_bldr.create_config('testfunc-alg', entry, version='1.0.1', worker_env=worker_env)\n",
    "alg_bldr.apply(compressed_alg_file=tarfilename, config=config)\n",
    "algs = alg_bldr.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline and store it\n",
    "fBuilder = PipelineBuilder(name='testfunc_pipe', api_server_base_url=api_server)\n",
    "fBuilder.add_node(node_name='testfunc_node', alg_name='testfunc-alg', input=[\"@flowInput.array\", \"@flowInput.dir\"])\n",
    "fBuilder.store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute stored pipeline\n",
    "fExec = PipelineExecutor(name='testfunc_pipe', api_server_base_url=api_server, tracker=TrackerType.POLLING)\n",
    "results = fExec.exec(input={\"array\": [23, 4, 12, 18, 7, 13, 40, 20], \"dir\": \"desc\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stored pipeline\n",
    "fBuilder.delete()\n",
    "alg_bldr.delete('testfunc-alg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build async\n",
    "worker_env = { \"WORKER_ALGORITHM_PROTOCOL\": \"ws\" }\n",
    "config = alg_bldr.create_config('other-alg', entry, version='1.0.2', worker_env=worker_env)\n",
    "state = alg_bldr.apply_async(compressed_alg_file=tarfilename, config=config)\n",
    "if 'buildId' in state.keys():\n",
    "    alg_bldr.get_build_state(state['buildId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Titanic Algs (by ds-alg-example project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create algorithm tar.gz file from github project:\n",
    "alg_mgr = AlgorithmBuilder(api_server_base_url=api_server)\n",
    "#tarfilename = alg_mgr.create_algfile_by_github('git@github.com:kube-HPC/ds-alg-example.git', 'algorithm')\n",
    "\n",
    "# alternatively create algorithm tar.gz file from local project folder:\n",
    "tarfilename = alg_mgr.create_algfile_by_folder('/home/yuvalso/anaconda3/Amir/ds-alg-example-master/algorithm')\n",
    "tarfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_env = { \n",
    "    \"WORKER_ALGORITHM_PROTOCOL\": \"ws\" ,\n",
    "    \"AWS_ACCESS_KEY_ID\": \"agambstorage\",\n",
    "    \"AWS_SECRET_ACCESS_KEY\": \"234eqndbpuCkGtH85KSyK/xAv3xuqdOpM3fKOLYlrSerpdKoG1FYy3kh6ArceL+yDwTvQOgs47xYO/ktnNzEeg==\",\n",
    "    \"S3_ENDPOINT_URL\": \"http://10.32.10.24:9000\"\n",
    "}\n",
    "\n",
    "\n",
    "# create titanicpp-alg\n",
    "folder = '/home/yuvalso/anaconda3/Amir/ds-alg-example-master/algorithm'\n",
    "tarfilename = alg_mgr.create_algfile_by_folder(folder)\n",
    "config = alg_mgr.create_config('titanicpp-alg', 'preprocess_entry.py', worker_env=worker_env, alg_env=alg_env)\n",
    "alg_mgr.apply(compressed_alg_file=tarfilename, config=config)\n",
    "algs = alg_mgr.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create titanicsplit-alg\n",
    "tarfilename = alg_mgr.create_algfile_by_folder(folder)\n",
    "config = alg_mgr.create_config('titanicsplit-alg', 'split_entry.py', worker_env=worker_env, alg_env=alg_env)\n",
    "alg_mgr.apply(compressed_alg_file=tarfilename, config=config)\n",
    "algs = alg_mgr.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create titanicparams-alg\n",
    "tarfilename = alg_mgr.create_algfile_by_folder(folder)\n",
    "config = alg_mgr.create_config('titanicparams-alg', 'params_entry.py', worker_env=worker_env, alg_env=alg_env)\n",
    "alg_mgr.apply(compressed_alg_file=tarfilename, config=config)\n",
    "algs = alg_mgr.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create titanicrf-alg\n",
    "tarfilename = alg_mgr.create_algfile_by_folder(folder)\n",
    "config = alg_mgr.create_config('titanicrf-alg', 'randomforest_entry.py', worker_env=worker_env, alg_env=alg_env)\n",
    "alg_mgr.apply(compressed_alg_file=tarfilename, config=config)\n",
    "algs = alg_mgr.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create titanicbestmodel-alg\n",
    "tarfilename = alg_mgr.create_algfile_by_folder(folder)\n",
    "config = alg_mgr.create_config('titanicbestmodel-alg', 'bestmodel_entry.py', worker_env=worker_env, alg_env=alg_env)\n",
    "alg_mgr.apply(compressed_alg_file=tarfilename, config=config)\n",
    "algs = alg_mgr.get_all()\n",
    "\n",
    "# NOTE: make sure all algorithms are included and docker images were created for them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Titanic Train Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tBuilder = PipelineBuilder(name='titanic-train2', api_server_base_url=api_server)\n",
    "tBuilder.add_node(node_name='preprocess', alg_name='titanicpp-alg', input=[\"@flowInput.df_key\"])\n",
    "tBuilder.add_node(node_name='split', alg_name='titanicsplit-alg', \n",
    "                  input=[{ \"df_key\": \"@preprocess.df_key\", \"test_size\": 0.25 }])\n",
    "tBuilder.add_node(node_name='model-params', alg_name='titanicparams-alg', \n",
    "                  input=[{\n",
    "                      \"param_and_range\": [\"min_samples_split\", [2, 3, 10]],\n",
    "                      \"params\": {\n",
    "                          \"n_estimators\": 10,\n",
    "                          \"max_depth\": 3\n",
    "                      }\n",
    "                  }])\n",
    "tBuilder.add_node(node_name='random-forest', alg_name='titanicrf-alg', \n",
    "                  input=[{\n",
    "                      \"params_combinations\": \"#@model-params\",\n",
    "                      \"x_train\": \"@split.x_train\",\n",
    "                      \"x_test\": \"@split.x_test\",\n",
    "                      \"y_train\": \"@split.y_train\",\n",
    "                      \"y_test\": \"@split.y_test\"\n",
    "                      }\n",
    "                  ])\n",
    "tBuilder.add_node(node_name='best-model', alg_name='titanicbestmodel-alg', \n",
    "                  input=[{\n",
    "                      \"df_key\": \"@preprocess.df_key\",\n",
    "                      \"models_results\": \"@random-forest\"\n",
    "                  }])\n",
    "tBuilder.get_raw()\n",
    "#time.sleep(1)\n",
    "tBuilder.store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Titanic Train Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRawExec = PipelineExecutor(raw=tBuilder.get_raw(), api_server_base_url=api_server, tracker=TrackerType.POLLING)\n",
    "results = tRawExec.exec(input={'df_key': 'train.csv'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Pipeline and Algorithms\n",
    "tBuilder.delete()\n",
    "alg_mgr.delete('titanicpp-alg')\n",
    "alg_mgr.delete('titanicsplit-alg')\n",
    "alg_mgr.delete('titanicparams-alg')\n",
    "alg_mgr.delete('titanicrf-alg')\n",
    "alg_mgr.delete('titanicbestmodel-alg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
